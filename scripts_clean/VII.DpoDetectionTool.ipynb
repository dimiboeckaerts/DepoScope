{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The final DpoDetection Tool :\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/concha-eloko/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at /media/concha-eloko/Linux/depolymerase_building/esm2_t12_35M_UR50D-finetuned-depolymerase/checkpoint-198/ were not used when initializing EsmForTokenClassification: ['esm.contact_head.regression.bias', 'esm.contact_head.regression.weight']\n",
      "- This IS expected if you are initializing EsmForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "path_work = \"/media/concha-eloko/Linux/depolymerase_building\"\n",
    "\n",
    "esm2_model_path = f\"{path_work}/esm2_t12_35M_UR50D-finetuned-depolymerase/checkpoint-198/\"\n",
    "DpoDetection_path = f\"{path_work}/DepoDetection.S1.conv.model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(esm2_model_path)\n",
    "esm2_finetuned = AutoModelForTokenClassification.from_pretrained(esm2_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dpo_classifier(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(Dpo_classifier, self).__init__()\n",
    "        self.max_length = 1024\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=5, stride=1)  \n",
    "        self.fc1 = nn.Linear(64 * (self.max_length - (5 - 1)), 32)  \n",
    "        self.classifier = nn.Linear(32, 2)\n",
    "\n",
    "    def make_prediction(self, fasta_txt):\n",
    "        input_ids = tokenizer.encode(fasta_txt, truncation=True, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            outputs = self.pretrained_model(input_ids)\n",
    "            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            token_probs, token_ids = torch.max(probs, dim=-1)            \n",
    "            tokens = token_ids.view(1, -1) \n",
    "            return tokens\n",
    "\n",
    "    def pad_or_truncate(self, tokens):\n",
    "        if tokens.size(1) < self.max_length:\n",
    "            tokens = F.pad(tokens, (0, self.max_length - tokens.size(1)))\n",
    "        elif tokens.size(1) > self.max_length:\n",
    "            tokens = tokens[:, :self.max_length]\n",
    "        return tokens\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        batch_size = len(sequences)\n",
    "        tokens_batch = []\n",
    "        for seq in sequences:\n",
    "            tokens = self.make_prediction(seq)\n",
    "            tokens = self.pad_or_truncate(tokens)\n",
    "            tokens_batch.append(tokens)\n",
    "        \n",
    "        outputs = torch.cat(tokens_batch).view(batch_size, 1, self.max_length)  \n",
    "        outputs = outputs.float()  \n",
    "        \n",
    "        out = F.relu(self.conv1(outputs))\n",
    "        out = out.view(batch_size, -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.classifier(out)\n",
    "        return out, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dpo_classifier(\n",
       "  (pretrained_model): EsmForTokenClassification(\n",
       "    (esm): EsmModel(\n",
       "      (embeddings): EsmEmbeddings(\n",
       "        (word_embeddings): Embedding(33, 480, padding_idx=1)\n",
       "        (position_embeddings): Embedding(1026, 480, padding_idx=1)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): EsmEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): EsmLayer(\n",
       "            (attention): EsmAttention(\n",
       "              (self): EsmSelfAttention(\n",
       "                (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (rotary_embeddings): RotaryEmbedding()\n",
       "              )\n",
       "              (output): EsmSelfOutput(\n",
       "                (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (intermediate): EsmIntermediate(\n",
       "              (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): EsmOutput(\n",
       "              (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): EsmLayer(\n",
       "            (attention): EsmAttention(\n",
       "              (self): EsmSelfAttention(\n",
       "                (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (rotary_embeddings): RotaryEmbedding()\n",
       "              )\n",
       "              (output): EsmSelfOutput(\n",
       "                (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (intermediate): EsmIntermediate(\n",
       "              (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): EsmOutput(\n",
       "              (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): EsmLayer(\n",
       "            (attention): EsmAttention(\n",
       "              (self): EsmSelfAttention(\n",
       "                (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (rotary_embeddings): RotaryEmbedding()\n",
       "              )\n",
       "              (output): EsmSelfOutput(\n",
       "                (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (intermediate): EsmIntermediate(\n",
       "              (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): EsmOutput(\n",
       "              (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): EsmLayer(\n",
       "            (attention): EsmAttention(\n",
       "              (self): EsmSelfAttention(\n",
       "                (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (rotary_embeddings): RotaryEmbedding()\n",
       "              )\n",
       "              (output): EsmSelfOutput(\n",
       "                (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (intermediate): EsmIntermediate(\n",
       "              (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): EsmOutput(\n",
       "              (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): EsmLayer(\n",
       "            (attention): EsmAttention(\n",
       "              (self): EsmSelfAttention(\n",
       "                (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (rotary_embeddings): RotaryEmbedding()\n",
       "              )\n",
       "              (output): EsmSelfOutput(\n",
       "                (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (intermediate): EsmIntermediate(\n",
       "              (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): EsmOutput(\n",
       "              (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): EsmLayer(\n",
       "            (attention): EsmAttention(\n",
       "              (self): EsmSelfAttention(\n",
       "                (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (rotary_embeddings): RotaryEmbedding()\n",
       "              )\n",
       "              (output): EsmSelfOutput(\n",
       "                (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (intermediate): EsmIntermediate(\n",
       "              (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): EsmOutput(\n",
       "              (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): EsmLayer(\n",
       "            (attention): EsmAttention(\n",
       "              (self): EsmSelfAttention(\n",
       "                (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (rotary_embeddings): RotaryEmbedding()\n",
       "              )\n",
       "              (output): EsmSelfOutput(\n",
       "                (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (intermediate): EsmIntermediate(\n",
       "              (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): EsmOutput(\n",
       "              (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): EsmLayer(\n",
       "            (attention): EsmAttention(\n",
       "              (self): EsmSelfAttention(\n",
       "                (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (rotary_embeddings): RotaryEmbedding()\n",
       "              )\n",
       "              (output): EsmSelfOutput(\n",
       "                (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (intermediate): EsmIntermediate(\n",
       "              (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): EsmOutput(\n",
       "              (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): EsmLayer(\n",
       "            (attention): EsmAttention(\n",
       "              (self): EsmSelfAttention(\n",
       "                (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (rotary_embeddings): RotaryEmbedding()\n",
       "              )\n",
       "              (output): EsmSelfOutput(\n",
       "                (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (intermediate): EsmIntermediate(\n",
       "              (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): EsmOutput(\n",
       "              (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): EsmLayer(\n",
       "            (attention): EsmAttention(\n",
       "              (self): EsmSelfAttention(\n",
       "                (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (rotary_embeddings): RotaryEmbedding()\n",
       "              )\n",
       "              (output): EsmSelfOutput(\n",
       "                (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (intermediate): EsmIntermediate(\n",
       "              (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): EsmOutput(\n",
       "              (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): EsmLayer(\n",
       "            (attention): EsmAttention(\n",
       "              (self): EsmSelfAttention(\n",
       "                (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (rotary_embeddings): RotaryEmbedding()\n",
       "              )\n",
       "              (output): EsmSelfOutput(\n",
       "                (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (intermediate): EsmIntermediate(\n",
       "              (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): EsmOutput(\n",
       "              (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): EsmLayer(\n",
       "            (attention): EsmAttention(\n",
       "              (self): EsmSelfAttention(\n",
       "                (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (rotary_embeddings): RotaryEmbedding()\n",
       "              )\n",
       "              (output): EsmSelfOutput(\n",
       "                (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (intermediate): EsmIntermediate(\n",
       "              (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): EsmOutput(\n",
       "              (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (emb_layer_norm_after): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (classifier): Linear(in_features=480, out_features=3, bias=True)\n",
       "  )\n",
       "  (conv1): Conv1d(1, 64, kernel_size=(5,), stride=(1,))\n",
       "  (fc1): Linear(in_features=65280, out_features=32, bias=True)\n",
       "  (classifier): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_classifier = Dpo_classifier(esm2_finetuned) # Create an instance of Dpo_classifier\n",
    "model_classifier.load_state_dict(torch.load(DpoDetection_path), strict = False) # Load the saved weights ; weird Error with some of the keys \n",
    "model_classifier.eval() # Set the model to evaluation mode for inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(model, sequence):\n",
    "    model.eval()  \n",
    "    with torch.no_grad():   \n",
    "        outputs, sequence_outputs = model([sequence])\n",
    "        probas = torch.nn.functional.softmax(outputs, dim=-1)\n",
    "        predictions = torch.argmax(probas, dim=1)  \n",
    "        sequence_outputs_list = sequence_outputs.cpu().numpy().tolist()[0][0]  \n",
    "        prob_predicted = probas[0][predictions].item()\n",
    "        return (predictions.item(), prob_predicted), sequence_outputs_list\n",
    "\n",
    "def plot_token(tokens) :\n",
    "    tokens = np.array(tokens)  # convert your list to numpy array for convenience\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for i in range(len(tokens) - 1):\n",
    "        if tokens[i] == 0:\n",
    "            color = 'black'\n",
    "        elif tokens[i] == 1:\n",
    "            color = 'blue'\n",
    "        else:\n",
    "            color = 'red'\n",
    "        plt.plot([i, i+1], [tokens[i], tokens[i+1]], color=color, marker='o')\n",
    "    plt.xlabel('Token')\n",
    "    plt.ylabel('Label')\n",
    "    plt.title('Label for each token')\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.yticks(np.arange(2), ['0', '1'])  \n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fasta = \"MGLHVQKSQLSVIKLGDYGAVGDGVTDDTTSFINLEAEHKGKIINLEFKTYLVDKGFSGNFYINGSFKVGDNTFAAPYTLPYANNSNIFMGENSGVNTDKYPVYMASAGGYSNIGIGKNALKSNTEGWRNVAIGDGALVNNTLGHYNIAVGDEALRDNIGSRNGDSTDNGSRNTAVGSNTMCYNTTGYCNTAMGRNALHTNFTGYHNTAIGAAALSGNAPYVNGVVVPDDPKHGNYNTAVGSEALFRGNSDHNTAVGRSAAWNTKNGARNVAIGSEALYYNEANVTYDDKTTAGAGNTAVGTAAMKYMQDGSQATLVNNSSAIGYGARVSGDNQVQLGGSGTTTYSYGAVQSRSDQRDKTDIKDTELGLDFLLKVRPVDFRWDYRDDYQEIDEEGNLITHEKDGSRSGNRFHHGVIAQEIQEVIQKTGKDFGGLQDHKINGGTDVLSIGYEEFIAPIIKSIHELHKMVSDLSDRISELENK\"\n",
    "\n",
    "prediction , token = predict_sequence(model_classifier ,input_fasta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((1, 0.9974331259727478), None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction  , plot_token(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# DpoDetectionTool BIG :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from Bio import SeqIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "esm2_model_path = \"/home/conchae/PhageDepo_pdb/script_files/models/esm2_t30_150M_UR50D-finetuned-depolymerase/checkpoint-198\"\n",
    "DpoDetection_path = f\"/home/conchae/PhageDepo_pdb/DepoDetection.t30_150M.conv2L.model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(esm2_model_path)\n",
    "esm2_finetuned = AutoModelForTokenClassification.from_pretrained(esm2_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dpo_classifier(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(Dpo_classifier, self).__init__()\n",
    "        self.max_length = 1026\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=5, stride=1)  # Convolutional layer 1 \n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, stride=1)  # Convolutional layer 2\n",
    "        self.fc1 = nn.Linear(128 * (self.max_length - 2 * (5 - 1)), 32)  # calculate the output shape after 2 conv layers\n",
    "        self.classifier = nn.Linear(32, 2)  # Binary classification\n",
    "\n",
    "    def make_prediction(self, fasta_txt):\n",
    "        input_ids = tokenizer.encode(fasta_txt, truncation=True, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            outputs = self.pretrained_model(input_ids)\n",
    "            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            token_probs, token_ids = torch.max(probs, dim=-1)            \n",
    "            tokens = token_ids.view(1, -1) # ensure 2D shape\n",
    "            return tokens\n",
    "\n",
    "    def pad_or_truncate(self, tokens):\n",
    "        if tokens.size(1) < self.max_length:\n",
    "            tokens = F.pad(tokens, (0, self.max_length - tokens.size(1)))\n",
    "        elif tokens.size(1) > self.max_length:\n",
    "            tokens = tokens[:, :self.max_length]\n",
    "        return tokens\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        batch_size = len(sequences)\n",
    "        tokens_batch = []\n",
    "        for seq in sequences:\n",
    "            tokens = self.make_prediction(seq)\n",
    "            tokens = self.pad_or_truncate(tokens)\n",
    "            tokens_batch.append(tokens)\n",
    "        \n",
    "        outputs = torch.cat(tokens_batch).view(batch_size, 1, self.max_length)  # ensure 3D shape\n",
    "        outputs = outputs.float()  # Convert to float\n",
    "        \n",
    "        out = F.relu(self.conv1(outputs))\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = out.view(batch_size, -1)  # Flatten the tensor\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.classifier(out)\n",
    "        return out, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classifier = Dpo_classifier(esm2_finetuned) # Create an instance of Dpo_classifier\n",
    "model_classifier.load_state_dict(torch.load(DpoDetection_path), strict = False) # Load the saved weights ; weird Error with some of the keys \n",
    "model_classifier.eval() # Set the model to evaluation mode for inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(model, sequence):\n",
    "    model.eval()  \n",
    "    with torch.no_grad():   \n",
    "        outputs, sequence_outputs = model([sequence])\n",
    "        probas = torch.nn.functional.softmax(outputs, dim=-1)\n",
    "        predictions = torch.argmax(probas, dim=1)  \n",
    "        sequence_outputs_list = sequence_outputs.cpu().numpy().tolist()[0][0]  \n",
    "        prob_predicted = probas[0][predictions].item()\n",
    "        return (predictions.item(), prob_predicted), sequence_outputs_list\n",
    "\n",
    "def plot_token(tokens) :\n",
    "    tokens = np.array(tokens)  # convert your list to numpy array for convenience\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for i in range(len(tokens) - 1):\n",
    "        if tokens[i] == 0:\n",
    "            color = 'black'\n",
    "        elif tokens[i] == 1:\n",
    "            color = 'blue'\n",
    "        else:\n",
    "            color = 'red'\n",
    "        plt.plot([i, i+1], [tokens[i], tokens[i+1]], color=color, marker='o')\n",
    "    plt.xlabel('Token')\n",
    "    plt.ylabel('Label')\n",
    "    plt.title('Label for each token')\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.yticks(np.arange(2), ['0', '1'])  \n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fasta = \"MGLHVQKSQLSVIKLGDYGAVGDGVTDDTTSFINLEAEHKGKIINLEFKTYLVDKGFSGNFYINGSFKVGDNTFAAPYTLPYANNSNIFMGENSGVNTDKYPVYMASAGGYSNIGIGKNALKSNTEGWRNVAIGDGALVNNTLGHYNIAVGDEALRDNIGSRNGDSTDNGSRNTAVGSNTMCYNTTGYCNTAMGRNALHTNFTGYHNTAIGAAALSGNAPYVNGVVVPDDPKHGNYNTAVGSEALFRGNSDHNTAVGRSAAWNTKNGARNVAIGSEALYYNEANVTYDDKTTAGAGNTAVGTAAMKYMQDGSQATLVNNSSAIGYGARVSGDNQVQLGGSGTTTYSYGAVQSRSDQRDKTDIKDTELGLDFLLKVRPVDFRWDYRDDYQEIDEEGNLITHEKDGSRSGNRFHHGVIAQEIQEVIQKTGKDFGGLQDHKINGGTDVLSIGYEEFIAPIIKSIHELHKMVSDLSDRISELENK\"\n",
    "\n",
    "prediction , token = predict_sequence(model_classifier ,input_fasta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Move the sequences to the server : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "rsync -avzhe ssh \\\n",
    "/media/concha-eloko/Linux/77_strains_phage_project/DetectedDpo.77_phages.multi.fasta \\\n",
    "conchae@garnatxa.srv.cpd:/home/conchae/PhageDepo_pdb \n",
    "    \n",
    "rsync -avzhe ssh \\\n",
    "/media/concha-eloko/Linux/77_strains_phage_project/MissedDpos.77_phages.multi.fasta \\\n",
    "conchae@garnatxa.srv.cpd:/home/conchae/PhageDepo_pdb \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_1 = \"/home/conchae/PhageDepo_pdb/DetectedDpo.77_phages.multi.fasta\"\n",
    "fasta_2 = \"/home/conchae/PhageDepo_pdb/MissedDpos.77_phages.multi.fasta\"\n",
    "\n",
    "path_fastas = [fasta_1 , fasta_2]\n",
    "\n",
    "with open(f\"/home/conchae/PhageDepo_pdb/77_Dpo_results.DepoDetection.t30_150M.conv2L.out\" , \"w\") as outfile :\n",
    "    for fasta in path_fastas :\n",
    "        outfile.write(f\"{fasta.split('/')[-1]}\\n\")\n",
    "        sequences = SeqIO.parse(fasta , \"fasta\")\n",
    "        for record in sequences : \n",
    "            prediction , token = predict_sequence(model_classifier , str(record.seq))\n",
    "            outfile.write(f\"{record.description}\\t{prediction[0]}__{prediction[1]}\\t{','.join(token)}\\n\")\n",
    "            outfile.write(f\"{record.description}\\t{str(prediction)}\\t{token}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#BATCH --job-name=big_pred__\n",
    "#SBATCH --qos=short \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=5\n",
    "#SBATCH --mem=50gb \n",
    "#SBATCH --time=1-00:00:00 \n",
    "#SBATCH --output=big_pred__%j.log \n",
    "\n",
    "source /storage/apps/ANACONDA/anaconda3/etc/profile.d/conda.sh\n",
    "conda activate embeddings\n",
    "\n",
    "\n",
    "python /home/conchae/PhageDepo_pdb/script_files/prediction_big.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
