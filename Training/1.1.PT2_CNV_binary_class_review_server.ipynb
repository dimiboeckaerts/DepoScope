{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer , AutoTokenizer\n",
    "\n",
    "import torch \n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "from Bio import SeqIO\n",
    "import os \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import warnings\n",
    "import subprocess\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "# ***********************************************************\n",
    "# Open the dataframes : \n",
    "path_work = \"/home/conchae/PhageDepo_pdb\n",
    "path_tmp = f\"{path_work}/low_cutoff_cluster\"\n",
    "os.makedirs(path_tmp, exist_ok=True)\n",
    "\n",
    "df_depo = pd.read_csv(f\"{path_work}/Phagedepo.Dataset.21032024.tsv\" , sep = \"\\t\" , header = 0)\n",
    "df_depo = df_depo[df_depo[\"Fold\"].isin([\"Negative\", \"right-handed beta-helix\", \"6-bladed beta-propeller\", \"triple-helix\"])]\n",
    "df_depo = df_depo.drop_duplicates(subset = [\"Full_seq\"], keep = \"first\")\n",
    "df_depo.reset_index(inplace = True)\n",
    "\n",
    "path_models = f\"{path_work}/script_files\"\n",
    "dico_path_models = {0.3 : f\"{path_models}/esm2_t12_35M_UR50D__0.3__finetuneddepolymerase.2205.4_labels/checkpoint-1185\"}\n",
    "                    #0.35 : f\"{path_models}/esm2_t12_35M_UR50D__0.7__finetuneddepolymerase.2103.4_labels/checkpoint-1945\",\n",
    "                    #0.4 : f\"{path_models}/esm2_t12_35M_UR50D__0.75__finetuneddepolymerase.2103.4_labels/checkpoint-1995\",\n",
    "                    #0.45 : f\"{path_models}/esm2_t12_35M_UR50D__0.8__finetuneddepolymerase.2103.4_labels/checkpoint-1980\",\n",
    "                    #0.5 : f\"{path_models}/esm2_t12_35M_UR50D__0.85__finetuneddepolymerase.2103.4_labels/checkpoint-1990\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***********************************************************\n",
    "thresholds = [0.3]\n",
    "\n",
    "def make_cluster_dico(cdhit_out) :\n",
    "    import json\n",
    "    dico_cluster = {}\n",
    "    threshold = cdhit_out.split(\"/\")[-1].split(\".out\")[0]\n",
    "    cluster_file = f\"{cdhit_out}.clstr\"\n",
    "    cluster_out = open(cluster_file).read().split(\">Cluster\")\n",
    "    for index,cluster in enumerate(cluster_out[1:]) :\n",
    "        tmp_dpo = []\n",
    "        #id_cluster = f\"Dpo_cdhit_{index}\"\n",
    "        id_cluster = index\n",
    "        for _,line in enumerate(cluster.split(\"\\n\")[1:-1]) :\n",
    "            dpo = line.split(\">\")[1].split(\".\")[0]\n",
    "            tmp_dpo.append(dpo)\n",
    "        dico_cluster[id_cluster] = tmp_dpo\n",
    "    with open(f\"{path_tmp}/dico_cluster.psicdhit__{threshold}.json\", \"w\") as outfile:\n",
    "        json.dump(dico_cluster, outfile)\n",
    "    return dico_cluster , threshold\n",
    "\n",
    "\n",
    "def reverse_dico(dico) : \n",
    "    r_dico = {}\n",
    "    for key,values in dico.items() :\n",
    "        for _,id in enumerate(values) : \n",
    "            r_dico[id] = key\n",
    "    return r_dico\n",
    "\n",
    "\n",
    "def make_list_group(list_seq, r_dico, id_dico) :\n",
    "    list_group = []\n",
    "    for _,seq in enumerate(list_seq) :\n",
    "        idd_seq = str(id_dico[seq])\n",
    "        list_group.append(r_dico[idd_seq])\n",
    "    return list_group\n",
    "\n",
    "\n",
    "def cvalue_to_list_group(threshold, df_depo) :\n",
    "    dico_cluster, _ = make_cluster_dico(f\"{path_tmp}/{threshold}__psicdhit.out\")\n",
    "    r_dico_cluster = reverse_dico(dico_cluster)\n",
    "    list_groups = make_list_group(df_depo[\"Full_seq\"].tolist(), r_dico_cluster, dico_seq_id)\n",
    "    return list_groups\n",
    "\n",
    "\n",
    "def get_labels(tuple_data ) :\n",
    "    dico_labels = {\"Negative\" : 0,\n",
    "                   \"right-handed beta-helix\" : 1,\n",
    "                   \"6-bladed beta-propeller\" : 2, \n",
    "                   \"triple-helix\" : 3}\n",
    "    labels_df = []\n",
    "    for _,row in enumerate(tuple_data) :\n",
    "        info = row[1]\n",
    "        seq_length = len(row[0])\n",
    "        fold = row[2]\n",
    "        label = dico_labels[fold]\n",
    "        if info == \"Negative\" :\n",
    "            labels = [label] * seq_length\n",
    "            labels_df.append(labels)\n",
    "        elif info == \"full_protein\" or info == \"full\" :\n",
    "            labels = [label] * seq_length\n",
    "            labels_df.append(labels)\n",
    "        elif info.count(\":\") > 0 : \n",
    "            start = int(info.split(\":\")[0])\n",
    "            end = int(info.split(\":\")[1])\n",
    "            labels = [0 if i < start or i >= end else label for i in range(seq_length)]\n",
    "            labels_df.append(labels)\n",
    "        else :\n",
    "            start = int(info.split(\"_\")[-2])\n",
    "            end = int(info.split(\"_\")[-1])\n",
    "            labels = [0 if i < start or i >= end else label for i in range(seq_length)]\n",
    "            labels_df.append(labels)\n",
    "    return labels_df\n",
    "\n",
    "def get_labels_seq(df) :\n",
    "    labels_df = []\n",
    "    for _,row in enumerate(df):\n",
    "        info = row[2]\n",
    "        seq_length = len(row[0])\n",
    "        if info == \"Negative\" :\n",
    "            label = 0\n",
    "            labels_df.append(label)         \n",
    "        else :\n",
    "            label = 1\n",
    "            labels_df.append(label)\n",
    "    return labels_df\n",
    "\n",
    "def training_data(threshold): \n",
    "    # Split the data : \n",
    "    gss_token_class = GroupShuffleSplit(n_splits=1, train_size=0.7, test_size = 0.3, random_state=243)\n",
    "    gss_seq_class = GroupShuffleSplit(n_splits=1, train_size=0.66, test_size = 0.34, random_state=243)\n",
    "    list_group_1 = cvalue_to_list_group(threshold, df_depo)\n",
    "    \n",
    "    # First split :\n",
    "    train_token_classification_indices = []\n",
    "    Other_indices = []\n",
    "    for i, (train_index, test_index) in enumerate(gss_token_class.split(df_depo[\"Full_seq\"], df_depo[\"Fold\"], list_group_1)):\n",
    "        train_token_classification_indices.append(train_index)\n",
    "        Other_indices.append(test_index)\n",
    "    \n",
    "    #train_tok_seq = df_depo[\"Full_seq\"][train_token_classification_indices[0]]\n",
    "    #train_tok_boundaries = df_depo[\"Boundaries\"][train_token_classification_indices[0]]\n",
    "    #train_tok_fold = df_depo[\"Fold\"][train_token_classification_indices[0]]\n",
    "    \n",
    "    #training_data_token_class = tuple(zip(train_tok_seq, train_tok_boundaries, train_tok_fold))\n",
    "    #training_data_tok_labels = get_labels(training_data_token_class)\n",
    "    \n",
    "    # Intermediate DF : \n",
    "    df_depo_s2 = df_depo[df_depo.index.isin(Other_indices[0])]\n",
    "    df_depo_s2.reset_index(inplace = True)\n",
    "    df_depo_s2\n",
    "    \n",
    "    # Second split : \n",
    "    list_group_2 = cvalue_to_list_group(threshold,df_depo_s2)\n",
    "    train_seq_classifiaction_indices = []\n",
    "    eval_data_indices = []\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(gss_seq_class.split(df_depo_s2[\"Full_seq\"], df_depo_s2[\"Fold\"], list_group_2)):\n",
    "        train_seq_classifiaction_indices.append(train_index)\n",
    "        eval_data_indices.append(test_index)\n",
    "    \n",
    "    train_cnv_seq = df_depo_s2[\"Full_seq\"][train_seq_classifiaction_indices[0]]\n",
    "    train_cnv_boundaries = df_depo_s2[\"Boundaries\"][train_seq_classifiaction_indices[0]]\n",
    "    train_cnv_fold = df_depo_s2[\"Fold\"][train_seq_classifiaction_indices[0]]\n",
    "    \n",
    "    # Sequence classification data :\n",
    "    training_data_cnv_class = tuple(zip(train_cnv_seq, train_cnv_boundaries, train_cnv_fold))\n",
    "    training_data_cnv_labels = get_labels_seq(training_data_cnv_class)\n",
    "    \n",
    "    # Ealuation data :\n",
    "    eval_seq = df_depo_s2[\"Full_seq\"][eval_data_indices[0]]\n",
    "    eval_seq_boundaries = df_depo_s2[\"Boundaries\"][eval_data_indices[0]]\n",
    "    eval_seq_fold = df_depo_s2[\"Fold\"][eval_data_indices[0]]\n",
    "    \n",
    "    eval_data_token_class = tuple(zip(eval_seq, eval_seq_boundaries, eval_seq_fold))\n",
    "    eval_data_token_labels = get_labels_seq(eval_data_token_class)\n",
    "\n",
    "    return train_cnv_seq, training_data_cnv_labels ,eval_seq , eval_data_token_labels\n",
    "\n",
    "def data_to_tensor(train_cnv_seq, training_data_cnv_labels ,eval_seq , eval_data_token_labels) :\n",
    "    Dataset_train_df = pd.DataFrame({\"sequence\" : list(train_cnv_seq) , \"Label\" : list(training_data_cnv_labels)})\n",
    "    Dataset_test_df = pd.DataFrame({\"sequence\" : list(eval_seq)  , \"Label\" : list(eval_data_token_labels)})\n",
    "    train_singledata = Dpo_Dataset(Dataset_train_df)\n",
    "    test_singledata = Dpo_Dataset(Dataset_test_df)\n",
    "    \n",
    "    train_single_loader = DataLoader(train_singledata, batch_size=12, shuffle=True, num_workers=4)\n",
    "    test_single_loader = DataLoader(test_singledata, batch_size=12, shuffle=True, num_workers=4)\n",
    "\n",
    "    return train_single_loader, test_single_loader\n",
    "\n",
    "class Dpo_Dataset(Dataset):\n",
    "    def __init__(self, Dataset_df):\n",
    "        self.sequence = Dataset_df.sequence.values\n",
    "        self.labels = torch.tensor(Dataset_df[\"Label\"].values, dtype=torch.long) \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item_domain1 = self.sequence[idx]\n",
    "        item_domain2 = self.labels[idx]\n",
    "        return item_domain1, item_domain2\n",
    "\n",
    "class Dpo_classifier(nn.Module):\n",
    "    def __init__(self, pretrained_model, tokenizer):\n",
    "        super(Dpo_classifier, self).__init__()\n",
    "        self.max_length = 1024\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=5, stride=1)  # Convolutional layer\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, stride=1)  # Convolutional layer\n",
    "        self.fc1 = nn.Linear(128 * (self.max_length - 2 * (5 - 1)), 32)  # calculate the output shape after 2 conv layers\n",
    "        self.classifier = nn.Linear(32, 1)  # Binary classification\n",
    "\n",
    "    def make_prediction(self,fasta_txt):\n",
    "        input_ids = self.tokenizer.encode(fasta_txt, truncation=True, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            outputs = self.pretrained_model(input_ids)\n",
    "            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            token_probs, token_ids = torch.max(probs, dim=-1)\n",
    "            tokens = token_ids.view(1, -1) # ensure 2D shape\n",
    "            return tokens\n",
    "\n",
    "    def pad_or_truncate(self, tokens):\n",
    "        if tokens.size(1) < self.max_length:\n",
    "            tokens = F.pad(tokens, (0, self.max_length - tokens.size(1)))\n",
    "        elif tokens.size(1) > self.max_length:\n",
    "            tokens = tokens[:, :self.max_length]\n",
    "        return tokens\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        batch_size = len(sequences)\n",
    "        tokens_batch = []\n",
    "        for seq in sequences:\n",
    "            tokens = self.make_prediction(seq)\n",
    "            tokens = self.pad_or_truncate(tokens)\n",
    "            tokens_batch.append(tokens)\n",
    "\n",
    "        outputs = torch.cat(tokens_batch).view(batch_size, 1, self.max_length)  # ensure 3D shape\n",
    "        outputs = outputs.float()  \n",
    "\n",
    "        out = F.relu(self.conv1(outputs))\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = out.view(batch_size, -1)  # Flatten the tensor\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.classifier(out)\n",
    "        return out, outputs\n",
    "\n",
    "\n",
    "def train_cnv(c_value, train_single_loader, test_single_loader) :\n",
    "    # get model\n",
    "    model_path = dico_path_models[c_value]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "    # Initialize model\n",
    "    model_classifier = Dpo_classifier(model,tokenizer)\n",
    "    model_classifier.train()\n",
    "    optimizer = optim.Adam(model_classifier.parameters(), lr=0.001) \n",
    "    criterion = nn.BCEWithLogitsLoss() \n",
    "    epochs = 5 \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model_classifier.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_correct = 0\n",
    "        total_samples = 0\n",
    "        for i, (sequences, labels) in enumerate(train_single_loader):\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs, _ = model_classifier(sequences)\n",
    "            loss = criterion(outputs.view(-1), labels.float()) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            predicted = (outputs > 0).float() \n",
    "            # Comipute accuracy\n",
    "            #_, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            epoch_correct += (predicted == labels).sum().item()\n",
    "            # Accumulate loss\n",
    "            epoch_loss += loss.item()\n",
    "        print(f'Epoch {epoch + 1}, Training Loss: {epoch_loss / len(train_single_loader):.4f}, Training Accuracy: {epoch_correct / total_samples:.4f}')\n",
    "        # Evaluation\n",
    "        model_classifier.eval()\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        with torch.no_grad():\n",
    "            for sequences, labels in test_single_loader:\n",
    "                outputs, _ = model_classifier(sequences)\n",
    "                predicted = (outputs > 0).float()\n",
    "                y_true.extend(labels.numpy())\n",
    "                y_pred.extend(predicted.numpy())            \n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred)  \n",
    "        recall = recall_score(y_true, y_pred)  \n",
    "        f1 = f1_score(y_true, y_pred)  \n",
    "        print(f'Testing Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}')\n",
    "        torch.save(model_classifier.state_dict(), f\"{path_work}/Deposcope__{c_value}__.esm2_t12_35M_UR50D.2205.review.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***********************************************************\n",
    "# Generate multifasta : \n",
    "dico_seq_id = {}\n",
    "for index, seq in enumerate(df_depo[\"Full_seq\"].tolist()) : \n",
    "    if seq not in dico_seq_id : \n",
    "        dico_seq_id[seq] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_training(c_value) : \n",
    "    # get training data :\n",
    "    model_path = dico_path_models[c_value]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "    train_cnv_seq, training_data_cnv_labels ,eval_seq , eval_data_token_labels = training_data(c_value)\n",
    "    train_single_loader, test_single_loader = data_to_tensor(train_cnv_seq, training_data_cnv_labels ,eval_seq , eval_data_token_labels)\n",
    "    train_cnv(c_value, train_single_loader, test_single_loader)\n",
    "\n",
    "for c_value in thresholds :\n",
    "    full_training(c_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#BATCH --job-name=cnv_review\n",
    "#SBATCH --qos=medium \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=30\n",
    "#SBATCH --mem=40gb \n",
    "#SBATCH --time=2-00:00:00 \n",
    "#SBATCH --output=cnv_review%j.log \n",
    "\n",
    "module restore la_base\n",
    "conda activate embeddings\n",
    "\n",
    "python /home/conchae/PhageDepo_pdb/script_files/cnv_training.review_v2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
