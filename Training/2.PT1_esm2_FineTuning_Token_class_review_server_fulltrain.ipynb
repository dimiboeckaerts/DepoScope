{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from transformers import AutoModelForTokenClassification,AutoTokenizer, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from evaluate import load\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "from Bio import SeqIO\n",
    "from collections import Counter\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***********************************************************\n",
    "# Open the dataframes : \n",
    "path_work = \"/home/conchae/PhageDepo_pdb\"\n",
    "path_tmp = f\"{path_work}/tmp\"\n",
    "os.makedirs(path_tmp, exist_ok=True)\n",
    "\n",
    "df_depo = pd.read_csv(f\"{path_work}/Phagedepo.Dataset.21032024.tsv\" , sep = \"\\t\" , header = 0)\n",
    "df_depo = df_depo[df_depo[\"Fold\"].isin([\"Negative\", \"right-handed beta-helix\", \"6-bladed beta-propeller\", \"triple-helix\"])]\n",
    "df_depo = df_depo.drop_duplicates(subset = [\"Full_seq\"], keep = \"first\")\n",
    "df_depo.reset_index(inplace = True)\n",
    "\n",
    "model_checkpoint = \"facebook/esm2_t12_35M_UR50D\"\n",
    "\n",
    "df_beta_helix = df_depo[df_depo[\"Fold\"] == \"right-handed beta-helix\"]\n",
    "df_beta_prope = df_depo[df_depo[\"Fold\"] == \"6-bladed beta-propeller\"]\n",
    "df_beta_triple =  df_depo[df_depo[\"Fold\"] == \"triple-helix\"]\n",
    "df_negative = df_depo[df_depo[\"Fold\"] == \"Negative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(df , label = 1) :\n",
    "    labels_df = []\n",
    "    for _,row in df.iterrows():\n",
    "        info = row[\"Boundaries\"]\n",
    "        seq_length = len(row[\"Full_seq\"])\n",
    "        if info == \"Negative\" :\n",
    "            labels = [label] * seq_length\n",
    "            labels_df.append(labels)\n",
    "        elif info == \"full_protein\" or info == \"full\" :\n",
    "            labels = [label] * seq_length\n",
    "            labels_df.append(labels)\n",
    "        elif info.count(\":\") > 0 : \n",
    "            start = int(info.split(\":\")[0])\n",
    "            end = int(info.split(\":\")[1])\n",
    "            labels = [0 if i < start or i >= end else label for i in range(seq_length)]\n",
    "            labels_df.append(labels)\n",
    "        else :\n",
    "            start = int(info.split(\"_\")[-2])\n",
    "            end = int(info.split(\"_\")[-1])\n",
    "            labels = [0 if i < start or i >= end else label for i in range(seq_length)]\n",
    "            labels_df.append(labels)\n",
    "    return labels_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********************************************\n",
    "# Transforming the data. Choose the label to associate with the triple helix :\n",
    "# Beta-helix :\n",
    "labels_beta_helix = get_labels(df_beta_helix , label = 1)\n",
    "seq_beta_helix = df_beta_helix[\"Full_seq\"].to_list()\n",
    "\n",
    "# Beta propeller : \n",
    "labels_beta_propeller = get_labels(df_beta_prope , label = 2)\n",
    "seq_beta_propeller = df_beta_prope[\"Full_seq\"].to_list()\n",
    "\n",
    "# Triple helix : \n",
    "labels_triple_helix = get_labels(df_beta_triple , label = 3)\n",
    "seq_triple_helix = df_beta_triple[\"Full_seq\"].to_list()\n",
    "\n",
    "# Negative :\n",
    "labels_negative = get_labels(df_negative , label = 0)\n",
    "seq_negative = df_negative[\"Full_seq\"].to_list()\n",
    "\n",
    "# The input data :\n",
    "sequences = seq_beta_helix + seq_beta_propeller + seq_triple_helix + seq_negative\n",
    "labels = labels_beta_helix + labels_beta_propeller + labels_triple_helix + labels_negative\n",
    "\n",
    "train_sequences, test_sequences, train_labels, test_labels = train_test_split(sequences, labels, test_size=0.2, random_state = 243)\n",
    "train_esm2 , train_CNV , esm2_labels , CNV_labels = train_test_split(train_sequences, train_labels, test_size=0.125, random_state = 243)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********************************************\n",
    "# Define the model :\n",
    "#model_checkpoint = \"facebook/esm2_t6_8M_UR50D\"\n",
    "model_checkpoint = \"facebook/esm2_t12_35M_UR50D\"\n",
    "#model_checkpoint = \"facebook/esm2_t30_150M_UR50D\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "train_tokenized = tokenizer(train_sequences)\n",
    "test_tokenized = tokenizer(test_sequences)\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_tokenized)\n",
    "test_dataset = Dataset.from_dict(test_tokenized)\n",
    "\n",
    "train_dataset = train_dataset.add_column(\"labels\", train_labels)\n",
    "test_dataset = test_dataset.add_column(\"labels\", test_labels)\n",
    "\n",
    "num_labels = 4\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********************************************\n",
    "# Training :\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}__fulltrain__finetuneddepolymerase.2103.{num_labels}_labels\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.001,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    logging_dir='./logs',\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "metric = load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    labels = labels.reshape((-1,))\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    predictions = predictions.reshape((-1,))\n",
    "    predictions = predictions[labels!=-100]\n",
    "    labels = labels[labels!=-100]\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#BATCH --job-name=full_token_class\n",
    "#SBATCH --qos=medium \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=20\n",
    "#SBATCH --mem=50gb \n",
    "#SBATCH --time=1-00:00:00 \n",
    "#SBATCH --output=full_token_class%j.log \n",
    "\n",
    "module restore la_base\n",
    "conda activate embeddings\n",
    "\n",
    "python /home/conchae/PhageDepo_pdb/script_files/esm2_finetuning.review.fulltrain.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_geometric",
   "language": "python",
   "name": "torch_geometric"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
