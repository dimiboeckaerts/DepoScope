{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from transformers import AutoModelForTokenClassification,AutoTokenizer, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from evaluate import load\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "from Bio import SeqIO\n",
    "from collections import Counter\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***********************************************************\n",
    "# Open the dataframes : \n",
    "path_work = \"/home/conchae/PhageDepo_pdb\n",
    "path_tmp = f\"{path_work}/tmp\"\n",
    "os.makedirs(path_tmp, exist_ok=True)\n",
    "\n",
    "df_depo = pd.read_csv(f\"{path_work}/Phagedepo.Dataset.21032024.tsv\" , sep = \"\\t\" , header = 0)\n",
    "df_depo = df_depo[df_depo[\"Fold\"].isin([\"Negative\", \"right-handed beta-helix\", \"6-bladed beta-propeller\", \"triple-helix\"])]\n",
    "df_depo = df_depo.drop_duplicates(subset = [\"Full_seq\"], keep = \"first\")\n",
    "df_depo.reset_index(inplace = True)\n",
    "\n",
    "model_checkpoint = \"facebook/esm2_t12_35M_UR50D\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***********************************************************\n",
    "# Token classification task,\u0000n\u0000 \u0000t\u0000a\u0000s\u0000k\u0000 \u0000t\u0000rs : \n",
    "thresholds = [0.65, 0.7, 0.75, 0.8, 0.85]\n",
    "\n",
    "def make_cdhit_cluster(threshold) :\n",
    "    cdhit_command = f\"cd-hit -i {path_tmp}/training_sequences.fasta -o {path_tmp}/{threshold}.out -c {threshold} -G 0 -aL 0.8\"\n",
    "    cdhit_process = subprocess.Popen(cdhit_command, shell =True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) \n",
    "    scan_out, scan_err = cdhit_process.communicate()\n",
    "    print(scan_out, scan_err)\n",
    "\n",
    "\n",
    "def make_cluster_dico(cdhit_out) :\n",
    "    import json\n",
    "    dico_cluster = {}\n",
    "    threshold = cdhit_out.split(\"/\")[-1].split(\".out\")[0]\n",
    "    cluster_file = f\"{cdhit_out}.clstr\"\n",
    "    cluster_out = open(cluster_file).read().split(\">Cluster\")\n",
    "    for index,cluster in enumerate(cluster_out[1:]) :\n",
    "        tmp_dpo = []\n",
    "        #id_cluster = f\"Dpo_cdhit_{index}\"\n",
    "        id_cluster = index\n",
    "        for _,line in enumerate(cluster.split(\"\\n\")[1:-1]) :\n",
    "            dpo = line.split(\">\")[1].split(\".\")[0]\n",
    "            tmp_dpo.append(dpo)\n",
    "        dico_cluster[id_cluster] = tmp_dpo\n",
    "    with open(f\"{path_tmp}/dico_cluster.cdhit__{threshold}.json\", \"w\") as outfile:\n",
    "        json.dump(dico_cluster, outfile)\n",
    "    return dico_cluster , threshold\n",
    "\n",
    "\n",
    "def reverse_dico(dico) : \n",
    "    r_dico = {}\n",
    "    for key,values in dico.items() :\n",
    "        for _,id in enumerate(values) : \n",
    "            r_dico[id] = key\n",
    "    return r_dico\n",
    "\n",
    "\n",
    "def make_list_group(list_seq, r_dico, id_dico) :\n",
    "    list_group = []\n",
    "    for _,seq in enumerate(list_seq) :\n",
    "        idd_seq = str(id_dico[seq])\n",
    "        list_group.append(r_dico[idd_seq])\n",
    "    return list_group\n",
    "\n",
    "\n",
    "def cvalue_to_list_group(threshold, df_depo) :\n",
    "    dico_cluster, _ = make_cluster_dico(f\"{path_tmp}/{threshold}.out\")\n",
    "    r_dico_cluster = reverse_dico(dico_cluster)\n",
    "    list_groups = make_list_group(df_depo[\"Full_seq\"].tolist(), r_dico_cluster, dico_seq_id)\n",
    "    return list_groups\n",
    "\n",
    "\n",
    "def get_labels(tuple_data ) :\n",
    "    dico_labels = {\"Negative\" : 0,\n",
    "                   \"right-handed beta-helix\" : 1,\n",
    "                   \"6-bladed beta-propeller\" : 2, \n",
    "                   \"triple-helix\" : 3}\n",
    "    labels_df = []\n",
    "    for _,row in enumerate(tuple_data) :\n",
    "        info = row[1]\n",
    "        seq_length = len(row[0])\n",
    "        fold = row[2]\n",
    "        label = dico_labels[fold]\n",
    "        if info == \"Negative\" :\n",
    "            labels = [label] * seq_length\n",
    "            labels_df.append(labels)\n",
    "        elif info == \"full_protein\" or info == \"full\" :\n",
    "            labels = [label] * seq_length\n",
    "            labels_df.append(labels)\n",
    "        elif info.count(\":\") > 0 : \n",
    "            start = int(info.split(\":\")[0])\n",
    "            end = int(info.split(\":\")[1])\n",
    "            labels = [0 if i < start or i >= end else label for i in range(seq_length)]\n",
    "            labels_df.append(labels)\n",
    "        else :\n",
    "            start = int(info.split(\"_\")[-2])\n",
    "            end = int(info.split(\"_\")[-1])\n",
    "            labels = [0 if i < start or i >= end else label for i in range(seq_length)]\n",
    "            labels_df.append(labels)\n",
    "    return labels_df\n",
    "\n",
    "def training_data(threshold): \n",
    "    # Split the data : \n",
    "    gss_token_class = GroupShuffleSplit(n_splits=1, train_size=0.7, test_size = 0.3, random_state=243)\n",
    "    gss_seq_class = GroupShuffleSplit(n_splits=1, train_size=0.66, test_size = 0.34, random_state=243)\n",
    "    list_group_1 = cvalue_to_list_group(threshold, df_depo)\n",
    "    \n",
    "    # First split :\n",
    "    train_token_classification_indices = []\n",
    "    Other_indices = []\n",
    "    for i, (train_index, test_index) in enumerate(gss_token_class.split(df_depo[\"Full_seq\"], df_depo[\"Fold\"], list_group_1)):\n",
    "        train_token_classification_indices.append(train_index)\n",
    "        Other_indices.append(test_index)\n",
    "    \n",
    "    train_tok_seq = df_depo[\"Full_seq\"][train_token_classification_indices[0]]\n",
    "    train_tok_boundaries = df_depo[\"Boundaries\"][train_token_classification_indices[0]]\n",
    "    train_tok_fold = df_depo[\"Fold\"][train_token_classification_indices[0]]\n",
    "    \n",
    "    training_data_token_class = tuple(zip(train_tok_seq, train_tok_boundaries, train_tok_fold))\n",
    "    training_data_tok_labels = get_labels(training_data_token_class)\n",
    "    \n",
    "    # Intermediate DF : \n",
    "    df_depo_s2 = df_depo[df_depo.index.isin(Other_indices[0])]\n",
    "    df_depo_s2.reset_index(inplace = True)\n",
    "    df_depo_s2\n",
    "    \n",
    "    # Second split : \n",
    "    list_group_2 = cvalue_to_list_group(threshold,df_depo_s2)\n",
    "    train_seq_classifiaction_indices = []\n",
    "    eval_data_indices = []\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(gss_seq_class.split(df_depo_s2[\"Full_seq\"], df_depo_s2[\"Fold\"], list_group_2)):\n",
    "        train_seq_classifiaction_indices.append(train_index)\n",
    "        eval_data_indices.append(test_index)\n",
    "    \n",
    "    train_seq_seq = df_depo_s2[\"Full_seq\"][train_seq_classifiaction_indices[0]]\n",
    "    train_seq_boundaries = df_depo_s2[\"Boundaries\"][train_seq_classifiaction_indices[0]]\n",
    "    train_seq_fold = df_depo_s2[\"Fold\"][train_seq_classifiaction_indices[0]]\n",
    "    \n",
    "    # Sequence classification data :\n",
    "    training_data_seq_class = tuple(zip(train_seq_seq, train_seq_boundaries, train_seq_fold))\n",
    "    training_data_seq_labels = get_labels(training_data_seq_class)\n",
    "    \n",
    "    # Ealuation data :\n",
    "    eval_seq = df_depo_s2[\"Full_seq\"][eval_data_indices[0]]\n",
    "    eval_seq_boundaries = df_depo_s2[\"Boundaries\"][eval_data_indices[0]]\n",
    "    eval_seq_fold = df_depo_s2[\"Fold\"][eval_data_indices[0]]\n",
    "    \n",
    "    eval_data_token_class = tuple(zip(eval_seq, eval_seq_boundaries, eval_seq_fold))\n",
    "    eval_data_token_labels = get_labels(eval_data_token_class)\n",
    "\n",
    "    return train_tok_seq, training_data_tok_labels ,eval_seq , eval_data_token_labels\n",
    "\n",
    "def mount_data(train_tok_seq, training_data_tok_labels ,eval_seq , eval_data_token_labels) :\n",
    "    # ***********************************************************\n",
    "    # Mount the data : \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    train_tokenized = tokenizer(list(train_tok_seq))\n",
    "    test_tokenized = tokenizer(list(eval_seq))\n",
    "    \n",
    "    train_dataset = Dataset.from_dict(train_tokenized)\n",
    "    test_dataset = Dataset.from_dict(test_tokenized)\n",
    "    \n",
    "    train_dataset = train_dataset.add_column(\"labels\", training_data_tok_labels)\n",
    "    test_dataset = test_dataset.add_column(\"labels\", eval_data_token_labels)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    metric = load(\"accuracy\")\n",
    "    predictions, labels = eval_pred\n",
    "    labels = labels.reshape((-1,))\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    predictions = predictions.reshape((-1,))\n",
    "    predictions = predictions[labels!=-100]\n",
    "    labels = labels[labels!=-100]\n",
    "    return metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***********************************************************\n",
    "# Generate multifasta : \n",
    "dico_seq_id = {}\n",
    "with open(f\"{path_tmp}/training_sequences.fasta\", \"w\") as outfile :\n",
    "    for index, seq in enumerate(df_depo[\"Full_seq\"].tolist()) : \n",
    "        outfile.write(f\">{index}\\n{seq}\\n\")\n",
    "        if seq not in dico_seq_id : \n",
    "            dico_seq_id[seq] = index\n",
    "\n",
    "for c_value in thresholds :\n",
    "    make_cdhit_cluster(c_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(c_value) : \n",
    "    # get training data :\n",
    "    train_tok_seq, training_data_tok_labels ,eval_seq , eval_data_token_labels = training_data(c_value)\n",
    "    train_dataset, test_dataset = mount_data(train_tok_seq, training_data_tok_labels ,eval_seq , eval_data_token_labels)\n",
    "    \n",
    "    # setup the training :\n",
    "    num_labels = 4\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "    model_name = model_checkpoint.split(\"/\")[-1]\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "    if os.path.isdir(f\"{model_name}__{c_value}__finetuneddepolymerase.2103.{num_labels}_labels\") == False :\n",
    "      # set args\n",
    "        batch_size = 4\n",
    "        args = TrainingArguments(\n",
    "            f\"{model_name}__{c_value}__finetuneddepolymerase.2103.{num_labels}_labels\",\n",
    "            evaluation_strategy = \"epoch\",\n",
    "            save_strategy = \"epoch\",\n",
    "            learning_rate=1e-5,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=5,\n",
    "            weight_decay=0.001,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            logging_dir='./logs',\n",
    "            push_to_hub=False,\n",
    "        )\n",
    "        # training :\n",
    "        metric = load(\"accuracy\")\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "            data_collator=data_collator,\n",
    "        )    \n",
    "        trainer.train()\n",
    "\n",
    "\n",
    "for c_value in thresholds[::-1] :\n",
    "    training_model(c_value)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#BATCH --job-name=token_class\n",
    "#SBATCH --qos=medium \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=20\n",
    "#SBATCH --mem=75gb \n",
    "#SBATCH --time=2-00:00:00 \n",
    "#SBATCH --output=token_class%j.log \n",
    "\n",
    "module restore la_base\n",
    "conda activate embeddings\n",
    "\n",
    "python /home/conchae/PhageDepo_pdb/script_files/esm2_finetuning.review.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_geometric",
   "language": "python",
   "name": "torch_geometric"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
