{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41d336c-f906-4943-b85e-8259139973da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score, matthews_corrcoef\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer , AutoTokenizer\n",
    "\n",
    "import torch \n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "from Bio import SeqIO\n",
    "import os \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import warnings\n",
    "import subprocess\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "# ***********************************************************\n",
    "# Open the dataframes : \n",
    "path_work = \"/home/conchae/PhageDepo_pdb\"\n",
    "path_tmp = f\"{path_work}/tmp\"\n",
    "os.makedirs(path_tmp, exist_ok=True)\n",
    "\n",
    "df_depo = pd.read_csv(f\"{path_work}/Phagedepo.Dataset.21032024.tsv\" , sep = \"\\t\" , header = 0)\n",
    "df_depo = df_depo[df_depo[\"Fold\"].isin([\"Negative\", \"right-handed beta-helix\", \"6-bladed beta-propeller\", \"triple-helix\"])]\n",
    "df_depo = df_depo.drop_duplicates(subset = [\"Full_seq\"], keep = \"first\")\n",
    "df_depo.reset_index(inplace = True)\n",
    "\n",
    "path_models = f\"{path_work}/script_files\"\n",
    "dico_path_models = {0.65 : f\"{path_models}/esm2_t12_35M_UR50D__0.65__finetuneddepolymerase.2103.4_labels/checkpoint-2105\",\n",
    "                    0.7 : f\"{path_models}/esm2_t12_35M_UR50D__0.7__finetuneddepolymerase.2103.4_labels/checkpoint-1945\",\n",
    "                    0.75 : f\"{path_models}/esm2_t12_35M_UR50D__0.75__finetuneddepolymerase.2103.4_labels/checkpoint-1995\",\n",
    "                    0.8 : f\"{path_models}/esm2_t12_35M_UR50D__0.8__finetuneddepolymerase.2103.4_labels/checkpoint-1980\",\n",
    "                    0.85 : f\"{path_models}/esm2_t12_35M_UR50D__0.85__finetuneddepolymerase.2103.4_labels/checkpoint-1990\"}\n",
    "\n",
    "dico_path_classifier = {0.65 : f\"{path_work}/Deposcope__0.65__.esm2_t12_35M_UR50D.2203.review.model\",\n",
    "                        0.7 : f\"{path_work}/Deposcope__0.7__.esm2_t12_35M_UR50D.2203.review.model\",\n",
    "                        0.75 : f\"{path_work}/Deposcope__0.75__.esm2_t12_35M_UR50D.2203.review.model\",\n",
    "                        0.8 : f\"{path_work}/Deposcope__0.8__.esm2_t12_35M_UR50D.2203.review.model\",\n",
    "                        0.85 : f\"{path_work}/Deposcope__0.85__.esm2_t12_35M_UR50D.2203.review.model\"}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a039c582-d024-4a09-943e-56b949aa21e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***********************************************************\n",
    "# Open the dataframes : \n",
    "path_work = \"/home/conchae/PhageDepo_pdb\"\n",
    "path_tmp = f\"{path_work}/tmp\"\n",
    "os.makedirs(path_tmp, exist_ok=True)\n",
    "\n",
    "df_depo = pd.read_csv(f\"{path_work}/Phagedepo.Dataset.21032024.tsv\" , sep = \"\\t\" , header = 0)\n",
    "df_depo = df_depo[df_depo[\"Fold\"].isin([\"Negative\", \"right-handed beta-helix\", \"6-bladed beta-propeller\", \"triple-helix\"])]\n",
    "df_depo = df_depo.drop_duplicates(subset = [\"Full_seq\"], keep = \"first\")\n",
    "df_depo.reset_index(inplace = True)\n",
    "\n",
    "df_beta_helix = df_depo[df_depo[\"Fold\"] == \"right-handed beta-helix\"]\n",
    "df_beta_prope = df_depo[df_depo[\"Fold\"] == \"6-bladed beta-propeller\"]\n",
    "df_beta_triple =  df_depo[df_depo[\"Fold\"] == \"triple-helix\"]\n",
    "df_negative = df_depo[df_depo[\"Fold\"] == \"Negative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d1506d-4ee5-42a5-8f8b-62ccb51faeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***********************************************************\n",
    "thresholds = [0.65, 0.7, 0.75, 0.8, 0.85]\n",
    "\n",
    "def make_cluster_dico(cdhit_out) :\n",
    "    import json\n",
    "    dico_cluster = {}\n",
    "    threshold = cdhit_out.split(\"/\")[-1].split(\".out\")[0]\n",
    "    cluster_file = f\"{cdhit_out}.clstr\"\n",
    "    cluster_out = open(cluster_file).read().split(\">Cluster\")\n",
    "    for index,cluster in enumerate(cluster_out[1:]) :\n",
    "        tmp_dpo = []\n",
    "        #id_cluster = f\"Dpo_cdhit_{index}\"\n",
    "        id_cluster = index\n",
    "        for _,line in enumerate(cluster.split(\"\\n\")[1:-1]) :\n",
    "            dpo = line.split(\">\")[1].split(\".\")[0]\n",
    "            tmp_dpo.append(dpo)\n",
    "        dico_cluster[id_cluster] = tmp_dpo\n",
    "    with open(f\"{path_tmp}/dico_cluster.cdhit__{threshold}.json\", \"w\") as outfile:\n",
    "        json.dump(dico_cluster, outfile)\n",
    "    return dico_cluster , threshold\n",
    "\n",
    "\n",
    "def reverse_dico(dico) : \n",
    "    r_dico = {}\n",
    "    for key,values in dico.items() :\n",
    "        for _,id in enumerate(values) : \n",
    "            r_dico[id] = key\n",
    "    return r_dico\n",
    "\n",
    "\n",
    "def make_list_group(list_seq, r_dico, id_dico) :\n",
    "    list_group = []\n",
    "    for _,seq in enumerate(list_seq) :\n",
    "        idd_seq = str(id_dico[seq])\n",
    "        list_group.append(r_dico[idd_seq])\n",
    "    return list_group\n",
    "\n",
    "\n",
    "def cvalue_to_list_group(threshold, df_depo) :\n",
    "    dico_cluster, _ = make_cluster_dico(f\"{path_tmp}/{threshold}.out\")\n",
    "    r_dico_cluster = reverse_dico(dico_cluster)\n",
    "    list_groups = make_list_group(df_depo[\"Full_seq\"].tolist(), r_dico_cluster, dico_seq_id)\n",
    "    return list_groups\n",
    "\n",
    "\n",
    "def get_labels(tuple_data ) :\n",
    "    dico_labels = {\"Negative\" : 0,\n",
    "                   \"right-handed beta-helix\" : 1,\n",
    "                   \"6-bladed beta-propeller\" : 2, \n",
    "                   \"triple-helix\" : 3}\n",
    "    labels_df = []\n",
    "    for _,row in enumerate(tuple_data) :\n",
    "        info = row[1]\n",
    "        seq_length = len(row[0])\n",
    "        fold = row[2]\n",
    "        label = dico_labels[fold]\n",
    "        if info == \"Negative\" :\n",
    "            labels = [label] * seq_length\n",
    "            labels_df.append(labels)\n",
    "        elif info == \"full_protein\" or info == \"full\" :\n",
    "            labels = [label] * seq_length\n",
    "            labels_df.append(labels)\n",
    "        elif info.count(\":\") > 0 : \n",
    "            start = int(info.split(\":\")[0])\n",
    "            end = int(info.split(\":\")[1])\n",
    "            labels = [0 if i < start or i >= end else label for i in range(seq_length)]\n",
    "            labels_df.append(labels)\n",
    "        else :\n",
    "            start = int(info.split(\"_\")[-2])\n",
    "            end = int(info.split(\"_\")[-1])\n",
    "            labels = [0 if i < start or i >= end else label for i in range(seq_length)]\n",
    "            labels_df.append(labels)\n",
    "    return labels_df\n",
    "\n",
    "def get_labels_seq(df) :\n",
    "    labels_df = []\n",
    "    for _,row in enumerate(df):\n",
    "        info = row[2]\n",
    "        seq_length = len(row[0])\n",
    "        if info == \"Negative\" :\n",
    "            label = 0\n",
    "            labels_df.append(label)         \n",
    "        else :\n",
    "            label = 1\n",
    "            labels_df.append(label)\n",
    "    return labels_df\n",
    "\n",
    "def training_data(threshold): \n",
    "    # Split the data : \n",
    "    gss_token_class = GroupShuffleSplit(n_splits=1, train_size=0.7, test_size = 0.3, random_state=243)\n",
    "    gss_seq_class = GroupShuffleSplit(n_splits=1, train_size=0.66, test_size = 0.34, random_state=243)\n",
    "    list_group_1 = cvalue_to_list_group(threshold, df_depo)\n",
    "    \n",
    "    # First split :\n",
    "    train_token_classification_indices = []\n",
    "    Other_indices = []\n",
    "    for i, (train_index, test_index) in enumerate(gss_token_class.split(df_depo[\"Full_seq\"], df_depo[\"Fold\"], list_group_1)):\n",
    "        train_token_classification_indices.append(train_index)\n",
    "        Other_indices.append(test_index)\n",
    "    \n",
    "    #train_tok_seq = df_depo[\"Full_seq\"][train_token_classification_indices[0]]\n",
    "    #train_tok_boundaries = df_depo[\"Boundaries\"][train_token_classification_indices[0]]\n",
    "    #train_tok_fold = df_depo[\"Fold\"][train_token_classification_indices[0]]\n",
    "    \n",
    "    #training_data_token_class = tuple(zip(train_tok_seq, train_tok_boundaries, train_tok_fold))\n",
    "    #training_data_tok_labels = get_labels(training_data_token_class)\n",
    "    \n",
    "    # Intermediate DF : \n",
    "    df_depo_s2 = df_depo[df_depo.index.isin(Other_indices[0])]\n",
    "    df_depo_s2.reset_index(inplace = True)\n",
    "    df_depo_s2\n",
    "    \n",
    "    # Second split : \n",
    "    list_group_2 = cvalue_to_list_group(threshold,df_depo_s2)\n",
    "    train_seq_classifiaction_indices = []\n",
    "    eval_data_indices = []\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(gss_seq_class.split(df_depo_s2[\"Full_seq\"], df_depo_s2[\"Fold\"], list_group_2)):\n",
    "        train_seq_classifiaction_indices.append(train_index)\n",
    "        eval_data_indices.append(test_index)\n",
    "    \n",
    "    train_cnv_seq = df_depo_s2[\"Full_seq\"][train_seq_classifiaction_indices[0]]\n",
    "    train_cnv_boundaries = df_depo_s2[\"Boundaries\"][train_seq_classifiaction_indices[0]]\n",
    "    train_cnv_fold = df_depo_s2[\"Fold\"][train_seq_classifiaction_indices[0]]\n",
    "    \n",
    "    # Sequence classification data :\n",
    "    training_data_cnv_class = tuple(zip(train_cnv_seq, train_cnv_boundaries, train_cnv_fold))\n",
    "    training_data_cnv_labels = get_labels_seq(training_data_cnv_class)\n",
    "    \n",
    "    # Ealuation data :\n",
    "    eval_seq = df_depo_s2[\"Full_seq\"][eval_data_indices[0]]\n",
    "    eval_seq_boundaries = df_depo_s2[\"Boundaries\"][eval_data_indices[0]]\n",
    "    eval_seq_fold = df_depo_s2[\"Fold\"][eval_data_indices[0]]\n",
    "    \n",
    "    eval_data_token_class = tuple(zip(eval_seq, eval_seq_boundaries, eval_seq_fold))\n",
    "    eval_data_token_labels = get_labels_seq(eval_data_token_class)\n",
    "\n",
    "    return train_cnv_seq, training_data_cnv_labels ,eval_seq , eval_data_token_labels\n",
    "\n",
    "def data_to_tensor(train_cnv_seq, training_data_cnv_labels ,eval_seq , eval_data_token_labels) :\n",
    "    Dataset_train_df = pd.DataFrame({\"sequence\" : list(train_cnv_seq) , \"Label\" : list(training_data_cnv_labels)})\n",
    "    Dataset_test_df = pd.DataFrame({\"sequence\" : list(eval_seq)  , \"Label\" : list(eval_data_token_labels)})\n",
    "    train_singledata = Dpo_Dataset(Dataset_train_df)\n",
    "    test_singledata = Dpo_Dataset(Dataset_test_df)\n",
    "    \n",
    "    train_single_loader = DataLoader(train_singledata, batch_size=12, shuffle=True, num_workers=4)\n",
    "    test_single_loader = DataLoader(test_singledata, batch_size=12, shuffle=True, num_workers=4)\n",
    "\n",
    "    return train_single_loader, test_single_loader\n",
    "\n",
    "class Dpo_Dataset(Dataset):\n",
    "    def __init__(self, Dataset_df):\n",
    "        self.sequence = Dataset_df.sequence.values\n",
    "        self.labels = torch.tensor(Dataset_df[\"Label\"].values, dtype=torch.long) \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item_domain1 = self.sequence[idx]\n",
    "        item_domain2 = self.labels[idx]\n",
    "        return item_domain1, item_domain2\n",
    "\n",
    "class Dpo_classifier(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(Dpo_classifier, self).__init__()\n",
    "        self.max_length = 1024\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=5, stride=1)  # Convolutional layer\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, stride=1)  # Convolutional layer\n",
    "        self.fc1 = nn.Linear(128 * (self.max_length - 2 * (5 - 1)), 32)  # calculate the output shape after 2 conv layers\n",
    "        self.classifier = nn.Linear(32, 1)  # Binary classification\n",
    "\n",
    "    def make_prediction(self, fasta_txt):\n",
    "        input_ids = self.tokenizer.encode(fasta_txt, truncation=True, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            outputs = self.pretrained_model(input_ids)\n",
    "            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            token_probs, token_ids = torch.max(probs, dim=-1)\n",
    "            tokens = token_ids.view(1, -1) # ensure 2D shape\n",
    "            return tokens\n",
    "\n",
    "    def pad_or_truncate(self, tokens):\n",
    "        if tokens.size(1) < self.max_length:\n",
    "            tokens = F.pad(tokens, (0, self.max_length - tokens.size(1)))\n",
    "        elif tokens.size(1) > self.max_length:\n",
    "            tokens = tokens[:, :self.max_length]\n",
    "        return tokens\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        batch_size = len(sequences)\n",
    "        tokens_batch = []\n",
    "        for seq in sequences:\n",
    "            tokens = self.make_prediction(seq)\n",
    "            tokens = self.pad_or_truncate(tokens)\n",
    "            tokens_batch.append(tokens)\n",
    "\n",
    "        outputs = torch.cat(tokens_batch).view(batch_size, 1, self.max_length)  # ensure 3D shape\n",
    "        outputs = outputs.float()  \n",
    "\n",
    "        out = F.relu(self.conv1(outputs))\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = out.view(batch_size, -1)  # Flatten the tensor\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.classifier(out)\n",
    "        return out, outputs\n",
    "\n",
    "\n",
    "def evaluate_deposcope(c_value, train_single_loader, test_single_loader) :\n",
    "    # Finetuned model\n",
    "    model_path = dico_path_models[c_value]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "    # Clasifier\n",
    "    model_classifier = Dpo_classifier(model) \n",
    "    model_classifier.load_state_dict(torch.load(dico_path_classifier[c_value]), strict = False) \n",
    "    model_classifier.eval()\n",
    "    # Initialize model\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in test_single_loader:\n",
    "            outputs, _ = model_classifier(sequences)\n",
    "            predicted = (outputs > 0).float()\n",
    "            y_true.extend(labels.numpy())\n",
    "            y_pred.extend(predicted.numpy())            \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)  \n",
    "    recall = recall_score(y_true, y_pred)  \n",
    "    f1 = f1_score(y_true, y_pred)  \n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    # Calculate specificity\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    with open(f\"{path_work}/review_metrics.Deposcope.tsv\", \"a+\") as outfile :\n",
    "        outfile.write(f\"{c_value}\\t{precision}\\t{recall}\\t{specificity}\\t{accuracy}\\t{f1}\\t{mcc}\\n\")\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f994025-55b1-4947-b033-02926e98bebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4c9503-b51e-496e-a20a-f2864b52ddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***********************************************************\n",
    "# Generate multifasta : \n",
    "dico_seq_id = {}\n",
    "for index, seq in enumerate(df_depo[\"Full_seq\"].tolist()) : \n",
    "    if seq not in dico_seq_id : \n",
    "        dico_seq_id[seq] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a58e0e4-ae0e-475b-8eb2-85c975681f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_eval(c_value) : \n",
    "    # get training data :\n",
    "    model_path = dico_path_models[c_value]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "    train_cnv_seq, training_data_cnv_labels ,eval_seq , eval_data_token_labels = training_data(c_value)\n",
    "    train_single_loader, test_single_loader = data_to_tensor(train_cnv_seq, training_data_cnv_labels ,eval_seq , eval_data_token_labels)\n",
    "    evaluate_deposcope(c_value, train_single_loader, test_single_loader)\n",
    "\n",
    "for c_value in thresholds :\n",
    "    full_eval(c_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a4d08a-3512-49c1-9d16-51b19b6a4c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#BATCH --job-name=metrics_review\n",
    "#SBATCH --qos=short \n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=16\n",
    "#SBATCH --mem=8gb \n",
    "#SBATCH --time=1-00:00:00 \n",
    "#SBATCH --output=metrics_review%j.log \n",
    "\n",
    "module restore la_base\n",
    "conda activate embeddings\n",
    "\n",
    "python /home/conchae/PhageDepo_pdb/script_files/deposcope_metrics.review.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf2ea6e-7db2-4e16-a52d-5489a7733f81",
   "metadata": {},
   "source": [
    "***\n",
    "### Plot the histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fda1c377-6539-4875-a476-11f6c3ee4005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_value</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.65</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.70</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.978648</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.957747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.995215</td>\n",
       "      <td>0.993007</td>\n",
       "      <td>0.989427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.80</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.995633</td>\n",
       "      <td>0.993007</td>\n",
       "      <td>0.989884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.99359</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.979701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   c_value  precision    recall  specificity  accuracy        f1       mcc\n",
       "0     0.65   1.000000  1.000000      1.00000  1.000000  1.000000  1.000000\n",
       "1     0.70   1.000000  0.953488      1.00000  0.978648  0.976190  0.957747\n",
       "2     0.75   1.000000  0.986111      1.00000  0.995215  0.993007  0.989427\n",
       "3     0.80   1.000000  0.986111      1.00000  0.995633  0.993007  0.989884\n",
       "4     0.85   0.986111  0.986111      0.99359  0.991228  0.986111  0.979701"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "path_work = \"/media/concha-eloko/Linux/PhageDEPOdetection\"\n",
    "\n",
    "col_names = [\"c_value\",\"precision\",\"recall\",\"specificity\",\"accuracy\",\"f1\",\"mcc\"]\n",
    "\n",
    "metrics_df = pd.read_csv(f\"{path_work}/benchmark.clustered_models.tsv\", sep = \"\\t\", names = col_names)\n",
    "#metrics_df.set_index(\"c_value\", inplace = True)\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac733b09-07cd-4ea6-bc50-29bae6194d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823c110d-9997-4d09-bfed-44384e4c6e04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_geometric",
   "language": "python",
   "name": "torch_geometric"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
